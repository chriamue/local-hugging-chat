version: '3'
services:

  nginx:
    image: nginx:latest
    ports:
      - "8000:80"
    volumes:
      - "./nginx.conf:/etc/nginx/conf.d/default.conf"
    depends_on:
      - chat-ui
      - text-generation-inference

  mongodb:
    image: mongo:latest

  chat-ui:
    image: ghcr.io/huggingface/chat-ui:latest
    container_name: chat-ui
    environment:
      - MONGODB_URL=mongodb://mongodb:27017
    ports:
      - "3000:3000"
    volumes:
      - ./.env:/app/.env.local
    depends_on:
      - mongodb
      - text-generation-inference

  text-generation-inference:
    image: ghcr.io/huggingface/text-generation-inference:1.2-rocm
    command: ["--max-total-tokens", "3080", "--max-input-length", "3072", "--max-batch-prefill-tokens", "3080", "--quantize", "awq", "--model-id", "mzbac/CodeLlama-34b-guanaco-awq"]
    volumes:
      - ./models:/data
    container_name: text-generation-inference
    devices:
      - "/dev/kfd"
      - "/dev/dri"
    shm_size: 1g
    ports:
      - "8080:80"

  chat-flame-backend:
    image: ghcr.io/chriamue/chat-flame-backend:main
    command: ["./chat-flame-backend", "--model", "phi-v2"]
    environment:
      - RUST_LOG=info
    volumes:
      - models:/tmp/models/

volumes:
  models: